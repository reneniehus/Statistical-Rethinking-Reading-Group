---
title: "Chapter 4 jonas"
output: html_notebook
---


#### Normality from addition
```{r}
pos <- replicate( 1000, sum( runif(4, -1, 1)))
plot(density((pos)))
```
#### Normality from multiplication
```{r}
library(rethinking)
growth <- replicate(10000, prod( 0.95 + runif(12,  0, 0.1)))
dens( growth, norm.comp=TRUE)
```

##### Multiplying small numbers is approx equal to addition!
```{r}
big_differences <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) ) 
small_differences <- replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )

hist(big_differences)
hist(small_differences)
```
##### Lareg deviates produce Gaussians on the log scale
```{r}
log_big <- log(big_differences)#replicate( 10000 , prod( 1 + runif(12,0,0.5) ) ) 
log_small <- log(small_differences)#replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )

hist(log_big)
hist(log_small)
```

#### Build a model

```{r}
data(Howell1)
d <- Howell1
#print(d)
d2 <- d[ d$age >= 18 , ]
```
now that we got the data, plot it!

```{r}
dens(d2$height)
```

```{r}
#plotting our priors
curve( dnorm( x, 178, 20), from=100, to=250)
```
```{r}
curve ( dunif(x, 0, 50), from=-10, to=60)
```
What do these priors imply? Let's sample from them!
```{r}
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
#dens( sample_mu )
#dens( sample_sigma )
dens(prior_h)
```
Mysterious! what exactly is rnorm doing here? He does not explain, and proceeds to ask you to simply run this code.
```{r}
mu.list <- seq( from=140, to=160 , length.out=200 ) 
sigma.list <- seq( from=4 , to=9 , length.out=200 ) 
post <- expand.grid( mu=mu.list , sigma=sigma.list ) 
post$LL <- sapply( 1:nrow(post) , function(i) sum( dnorm( d2$height , mean=post$mu[i] , sd=post$sigma[i] , log=TRUE ) ) )
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) + dunif( post$sigma , 0 , 50 , TRUE )
post$prob <- exp( post$prod - max(post$prod) )

image_xyz( post$mu , post$sigma , post$prob )

contour_xyz( post$mu , post$sigma , post$prob, add=TRUE )


```
Sample from this
```{r}
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE , prob=post$prob )
sample.mu <- post$mu[ sample.rows ] 
sample.sigma <- post$sigma[ sample.rows ]
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
```
describe these samples from the posterior just as if they were data; e.g. the _marginal_ posterior densities of \mu and \sigma
```{r}
dens( sample.mu )
dens( sample.sigma )
```
of via HPDI

```{r}
HPDI( sample.mu ) 
HPDI( sample.sigma )
```

##### Overthinking
\sigma causes trouble cause it must be positive and is thus not 'even'. Use fewer data points to demonstrate.
```{r}
d3 <- sample( d2$height , size=20 )

mu.list <- seq( from=150, to=170 , length.out=200 ) 
sigma.list <- seq( from=4 , to=20 , length.out=200 ) 
post2 <- expand.grid( mu=mu.list , sigma=sigma.list ) 
post2$LL <- sapply( 1:nrow(post2) , function(i) sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] , log=TRUE ) ) )
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) + dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob <- exp( post2$prod - max(post2$prod) ) 
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob )
sample2.mu <- post2$mu[ sample2.rows ] 
sample2.sigma <- post2$sigma[ sample2.rows ] 
plot( sample2.mu , sample2.sigma , cex=0.5 , col=col.alpha(rangi2,0.1) , xlab="mu" , ylab="sigma" , pch=16 )
#marginal posterior density: shows it's not normal, long tail to larger values
dens( sample2.sigma , norm.comp=TRUE )
```

#### Quadric approximation
leverage the fact/ the assumption that about the MAP, the distribution will be somewhat like a quadric function. The function map finds a probability for each parameter combo, and climbs the posterior to find the MAP peak.
```{r}
# load data
library(rethinking)
data("Howell1")
d <- Howell1
d2 <- d[ d$age >= 18 , ]
```
Define the model
```{r}
flist <- alist(
  height ~ dnorm( mu, sigma ),
  mu ~ dnorm( 178, 20 ),
  sigma ~ dunif( 0, 50 )
)
```
Fit
```{r}
m4.1 <- map( flist, data=d2 )
```
MAP model:
```{r}
precis( m4.1 )
```
with narrower prior for the mean
```{r}
m4.2 <- map(
  alist(
    height ~ dnorm( mu , sigma ) , 
    mu ~ dnorm( 178 , 1 ) , 
    sigma ~ dunif( 0 , 50 )
    ) , 
  data=d2 )
precis( m4.2 )
```
```{r}
# covariance matrix for the model; needed to construct multinomial gaussian for generating samples from posterior
vcov( m4.1 )
diag( vcov( m4.1 ) )
cov2cor( vcov( m4.1 ) )
```
```{r}
library(rethinking)
post <- extract.samples( m4.1 , n=1e4)
head(post)
precis(post)
precis(m4.1)
```
```{r}
library(MASS) 
postmvnorm <- mvrnorm( n=1e4 , mu=coef(m4.1) , Sigma=vcov(m4.1) )
head(postmvnorm)
```
##### Overthinking - log transform of sigma
```{r}
m4.1_logsigma <- map( 
  alist(
    height ~ dnorm( mu , exp(log_sigma) ) , 
    mu ~ dnorm( 178 , 20 ) , 
    log_sigma ~ dnorm( 2 , 10 )
    ) , data=d2 )

post <- extract.samples( m4.1_logsigma ) 
sigma <- exp( post$log_sigma )
dens(post)
dens(sigma)

```
#### Now make it a linear regression!
```{r}
#plot to get a feel for correllation
plot( d2$height ~ d2$weight)
```
```{r}
# fitting the model
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18, ]

#fit
m4.3 <- map(
  alist(
    height ~dnorm( mu, sigma ) ,
    mu <- a + b*weight ,
    a ~ dnorm( 178, 100), 
    b ~ dnorm( 0, 10 ),
    sigma ~ dunif( 0, 50 )
  ) ,
  data=d2 )

```
inspect estimates
```{r}
precis( m4.3 )
print(mean(d2$weight))
```
the above is insufficient to describe the posterior completely; for that we need the variance-covariance matrix
```{r}
precis( m4.3 , corr=TRUE)
```
now we see that a and b are almost perfectly negatively correlated, this can lead
 to problems in finding an optimum in more complicated inference problems. One way to begin to ameliorate this is to center the data.
 
```{r}
d2$weight.c <- d2$weight - mean(d2$weight)

m4.4 <- map(
  alist(
    height ~dnorm( mu, sigma ) ,
    mu <- a + b*weight.c ,
    a ~ dnorm( 178, 100), 
    b ~ dnorm( 0, 10 ),
    sigma ~ dunif( 0, 50 )
  ) ,
  data=d2 )

precis(m4.4 , corr=TRUE)

```
#### Plotting and visualizing

```{r}
plot( height ~ weight, data=d2)
abline( a=coef(m4.3)["a"] , b=coef(m4.3)["b"])
```
To better appreciate how the posterior distribution contains lines, extract some samples
from the model:
```{r}
post <- extract.samples( m4.3 )
print(post[1:5, ])
```
```{r}
# recalculate the model with fewer data
N<-352
dN <- d2[ 1:N , ] 
mN <- map( alist(
  height ~ dnorm( mu , sigma ) , 
  mu <- a + b*weight , 
  a ~ dnorm( 178 , 100 ) , 
  b ~ dnorm( 0 , 10 ) , 
  sigma ~ dunif( 0 , 50 )
  ), data=dN)
# extract 20 samples from the posterior
post <- extract.samples(mN, n=N)
# display raw data and sample size
plot( dN$weight, dN$height , 
      xlim=range(d2$weight), ylim=range(d2$height),
      col=rangi2, xlab="weight", ylab="height")
mtext(concat("N = ", N))
# plot the lines, with transparency
for ( i in 1:20)
  abline( a=post$a[i], b=post$b[i], col=col.alpha("black", 0.3))
```
how to make this into a shaded 'confidence' region? Focus on a hypthetical individual that weighs 50kg and draw from posterior
```{r}
mu_at_50 <- post$a + post$b*50
print(post$a)
# since joint a and b went into computing each of these posterior samples for an individual weighing 50kg, the variation across those means incorporates the uncertainty in and correlation between both parameters. plot it
dens(mu_at_50, col=rangi2, lwd=2, xlab="mu|weigh=50kg")
```
since mu has a distribution, we can calculate intervals from it
```{r}
HPDI(mu_at_50, prob=0.89)
```
do so for each weight value
```{r}
# this draws 1000 samples (rows) for each individual (count 352)
#mu <- link(m4.3)
#str(mu)

# to do this for each weight:
# define sequence of weights to compute predictions for these values will be on the 
# horizontal axis
weight.seq <- seq( from=25, to=70, by=1)
# use link to compute mu
# for each sample from the posterior
# and for each weight in weight.seq
mu <- link(m4.3, data=data.frame(weight=weight.seq))
str(mu)

```
```{r}
#use type="n" to hide raw data
plot( height ~ weight , d2, type="n")

#loop over samples and plot each mu value
for ( i in 1:100)
  points( weight.seq , mu[i, ] , pch=16, col=col.alpha(rangi2, 0.1) )
```
```{r}
mu.mean <- apply( mu, 2, mean )
mu.HPDI <- apply( mu, 2, HPDI, prob=0.89)
#plot raw data
# dading out points to make line and interval more visible
plot( height ~ weight, data=d2, col=col.alpha(rangi2, 0.5))
#plot the MAP line, aka the mean mu for each weight
lines( weight.seq, mu.mean)
#plot a shaded region for 89% HPDI
shade(mu.HPDI, weight.seq)
```
incorporating the uncertainty in the gaussian likelihood of the height (the sigma that I (probably incorrectly) call the residual irreducible error)
```{r}
sim.height <- sim(m4.3 , data=list(weight=weight.seq))
str(sim.height)
```
summarize just like before
```{r}
height.PI97 <- apply( sim.height, 2, PI, prob=0.97 )
height.PI <- apply( sim.height, 2, PI, prob=0.89 )
height.PI67 <- apply( sim.height, 2, PI, prob=0.67 )
#contains the 89% posterior prediction interval of observable (according to the model) heights, across the values of weight in weight.seq
```
plot
```{r}
# plot raw data 
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )
# draw MAP line 
lines( weight.seq , mu.mean )
# draw HPDI region for line 
shade( mu.HPDI , weight.seq )
# draw PI 97% region for simulated heights 
shade( height.PI97 , weight.seq )
# draw PI region for simulated heights 
shade( height.PI , weight.seq )
# draw PI 67% region for simulated heights 
shade( height.PI67 , weight.seq )
```
#### Polynomial regression
```{r}
library(rethinking)
data(Howell1)
d <- Howell1
str(d)
# height vs weight is visibly curved
plot(d$height, d$weight)
```












